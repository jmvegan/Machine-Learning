summary(logRegshuttle)
exp(logRegshuttle$coef)
exp(0.6952323 +  (0.9684981*2)/exp(0.6952323 +  0.9684981)
exp(0.6952323 +  (0.9684981*2))/exp(0.6952323 +  0.9684981)
2.633986/2
binUse <- as.numeric(shuttle$use) - 1;
logRegshuttle <- glm(binUse ~ wind, data=shuttle, family="binomial")
summary(logRegshuttle)
exp(logRegshuttle$coef)
binUseInverted <- 1 - binUse
logRegshuttle <- glm(binUseInverted ~ wind, data=shuttle, family="binomial")
summary(logRegshuttle)
exp(logRegshuttle$coef)
library InsectSprays
library (InsectSprays)
library(InsectSprays)
InsectSprays
test <- glm(count~spray, y=InsectSprays, family=poisson)
test <- glm(data=InsectSprays, family=poisson)
test <- glm(spray, data=InsectSprays, family=poisson)
test <- glm(sum(spray), data=InsectSprays, family=poisson)
test <- sum(count$spray=A)
log(10)
test <- glm(count~x, y=InsectSprays, family=poisson)
lm(count ~ spray, offset(blah))
blah=1
lm(count ~ spray, offset(blah))
lm(count ~ spray, data=InsectSprays, offset(blah))
lm(count ~ spray, data=InsectSprays)
lm(count ~ spray+1, data=InsectSprays)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
lhs<-function(x) ifelse(x< K,K-x,0)
rhs<-function(x) ifelse(x>K,x-K,0)
lhs+rhs
lhs+rhs
4.97/5
1/1.327
library(kernlab)
install.packages("kernlab")
data(spam)
head(spam)
libary(kernlab)
library(kernlab)
head(spam)
data(spam)
head(spam)
prediction <-ifelse(spam$your > 0.5, "spam", "nonspam")
table(prediction,spam$type)/length(spam$type)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
install.packages("caret")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
install.packages("lattice")
install.packages("ggplot2")
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
install.packages("caret")
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
createDataPartition?
?createDataPartition
??createDataPartition
library(caret)
library(lattice)
library(ggplot2)
library(caret)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(testing)
View(training)
install.packages("Hmisc")
View(training)
View(testing)
View(training)
featurePlot(x=training[,c("Cement", "BlastFurnaceSlag", "FlyAsh", "Water","Superplasticizer","CoarseAggregate","FineAggregate", "Age")],y=training$CompressiveStrength,plot="pairs"
)
featurePlot(x=training[,c("Age")],y=training$CompressiveStrength,plot="pairs")
qplot(Age, CompressiveStrength, data=training)
qplot(FlyAsh, CompressiveStrength, data=training)
cutWage <-cut2(training$CompressiveStrength, g=3)
library(hmisc)
library(Hmisc)
install.packages("gridBase")
install.packages("survival")
install.packages("splines")
library(Hmisc)
install.packages("Hmisc")
plot(CompressiveStrength)
plot(training$CompressiveStrength)
testing = mixtures[-inTrain,]
training = mixtures[!rownames(mixtures) %in% rownames(testing),]
plot(training$CompressiveStrength)
sapply(training[,1:8],cut2,g=4) -> training[,1:8]
install.packages("Hmisc")
library(cut2)
library(hmisc)
install.packages("hmisc")
cut2()
Hmisc
install.packages("Hmisc")
library(Hmisc)
force(Hmisc)
sapply(training[,1:8],cut2,g=4) -> training[,1:8]
featurePlot(x=training[,1:8],y=training$CompressiveStrength, plot="pairs")
libary(caret)
install.packages("caret")
featurePlot(x=training[,1:8],y=training$CompressiveStrength, plot="pairs")
par(mfcol=c(2,4))
for (i in 1:8) {plot(training$CompressiveStrength,main=names(training[i]),xlab="stepwise index",ylab="CompressiveStrength",col=training[,i])}
for (i in 1:8) {plot(training$CompressiveStrength,main=names(training[i]),xlab="stepwise index",ylab="CompressiveStrength",col=training[,i])}
for (i in 1:8) {plot(training$CompressiveStrength,main=names(training[i]),xlab="stepwise index",ylab="CompressiveStrength",col=i)}
sapply(training[,1:8],cut2,g=4) -> training[,1:8]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
install.packages("ggplot2")
library(caret)
install.packages("lattice")
install.packages("caret")
library(caret)
library(lattice)
library(ggplot2)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(Hmisc)
install.packages("gridBase")
install.packages("survival")
install.packages("splines")
install.packages("Hmisc")
library(Hmisc)
library(grid)
library(lattice)
library(survival)
library(splines)
library(Formula)
library(Hmisc)
sapply(training[,1:8],cut2,g=4) -> training[,1:8]
for(i in 1:8){
legend("topright",legend=levels(training[,i]),
col=training[,i],pch=1)
}
par(mfcol=c(2,4))
for (i in 1:8) {plot(training$CompressiveStrength,main=names(training[i]),xlab="stepwise index",ylab="CompressiveStrength",col=training[,i])}
for (i in 1:8) {plot(training$CompressiveStrength,main=names(training[i]),xlab="stepwise index",ylab="CompressiveStrength",col=training[,i])}
for (i in 1:8) {plot (index)(training$CompressiveStrength,main=names(training[i]),xlab="stepwise index",ylab="CompressiveStrength",col=training[,i])}
plot(training$Age,training$CompressiveStrength)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
ibrary(ggplot2)
library(ggplot2)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
preObj <- preProcess(training[,-58], method=c("BoxCox"))
hist(training)
hist(training$Superplasticizer)
hist(training$Superplasticizer+1)
library(ggplot2);
install.packages("ggplot2")
library(caret)
install.packages("lattice")
library(caret)
library(ggplot2)
library(lattice)
library(caret)
install.packages("randomForest")
library(ramdomForest)
library(randomForest)
install.packages("Hmisc")
library(Hmisc)
library(grid)
library(survival)
library(splines)
install.packages("Hmisc")
install.packages("Hmisc")
library(Hmisc)
library(Formula)
library(Hmisc)
library(foreach)
library(doParallel)
install.packages("doParallel")
library(doParallel)
library(iterators)
library(parallel)
library(doParallel)
options(warn=1)
options(stringsAsFactors = FALSE)
setdirectory
setwd
setwd("~/Documents/Coursera/Data Science Johns Hopkins/Machine Learning")
testing <-read.csv("pml-testing.csv")
training <- read.csv("pml-training.csv")
training
dim(training)
dim(testing)
head(training)
tail(training)
tail(testing)
head(training)
set.seed(123)
training <- training[,union(grep("^accel_", colnames(training)),grep("classe",colnames(training)) )]
testing <- testing[,union(grep("^accel_", colnames(testing)),grep("classe",colnames(testing)) )]
names(training)
partition <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
partition <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
library(caret)
libary(cluster)
library(cluster)
library(survival)
library(ggplot2)
library(caret)
partition <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
sample1 <- training[partition, ]
psample1 <-  training[-partition, ]
sample1$classe <-as.factor(sample1$classe)
sample1[, 1:6] <- sapply(sample1[, 1:6], as.numeric)
psample1$classe <-as.factor(psample1$classe)
psample1[, 1:6] <- sapply(psample1[, 1:6], as.numeric)
registerDoParallel()
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)
}
library(Hmisc)
library(randomForest)
registerDoParallel()
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)
}
registerDoParallel()
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)}
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)}
sample1$classe <-as.factor(sample1$classe)
sample1[, 1:6] <- sapply(sample1[, 1:6], as.numeric)
psample1$classe <-as.factor(psample1$classe)
psample1[, 1:6] <- sapply(psample1[, 1:6], as.numeric)
registerDoParallel()
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)}
psample1[, 1:6]
library(foreach)
randomForest(x = x, y = y, ntree = ntree)
rf <- foreach(ntree=rep(150, 6), .combine=combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)}
sample1$classe
set.seed(123)
training <- training[,union(grep("^accel_", colnames(training)),grep("classe",colnames(training)) )]
testing <- testing[,union(grep("^accel_", colnames(testing)),grep("classe",colnames(testing)) )]
names(training)
partition <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
sample1 <- training[partition, ]
psample1 <-  training[-partition, ]
sample1$classe <-as.factor(sample1$classe)
sample1[, 1:6] <- sapply(sample1[, 1:6], as.numeric)
psample1$classe <-as.factor(psample1$classe)
psample1[, 1:6] <- sapply(psample1[, 1:6], as.numeric)
sample1$classe
training <- read.csv("pml-training.csv")
dim(training)
set.seed(123)
training <- training[,union(grep("^accel_", colnames(training)),grep("classe",colnames(training)) )]
testing <- testing[,union(grep("^accel_", colnames(testing)),grep("classe",colnames(testing)) )]
names(training)
partition <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
sample1 <- training[partition, ]
psample1 <-  training[-partition, ]
sample1$classe <-as.factor(sample1$classe)
sample1[, 1:6] <- sapply(sample1[, 1:6], as.numeric)
psample1$classe <-as.factor(psample1$classe)
psample1[, 1:6] <- sapply(psample1[, 1:6], as.numeric)
sample1$classe
registerDoParallel()
rf <- foreach(ntree=rep(150, 6), .combine=combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)}
pred0 <- predict(rf,psample1)
confusionMatrix(pred0,psample1$classe)
install.packages("Hmisc")
library(Hmisc)
pred0 <- predict(rf,psample1)
confusionMatrix(pred0,psample1$classe)
pred0 <- predict(rf,psample1)
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)
}
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)}
registerDoParallel()
registerDoParallel()
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)}
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)
}
registerDoParallel()
library(ggplot2)
library(caret)
library(randomForest)
library(Hmisc)
library(foreach)
library(doParallel)
library(ggplot2)
library(caret)
library(cluster)
library(survival)
library(randomForest)
library(Hmisc)
library(foreach)
library(iterators)
library(parallel)
library(doParallel)
registerDoParallel()
rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)
}
pred0 <- predict(rf,psample1)
confusionMatrix(pred0,psample1$classe)
install.packages('e1071', dependencies=TRUE)
confusionMatrix(pred0,psample1$classe)
predict(rf, testing)
rf <- foreach(ntree=rep(150, 6), .combine=combine, .packages='randomForest') %dopar% {
randomForest(x=s_training[,1:12], y=s_training$classe, ntree=ntree)
}
prediction1 <- predict(rf,s_test)
library(ggplot2)
library(caret)
library(cluster)
library(survival)
library(randomForest)
library(Hmisc)
library(foreach)
library(iterators)
library(parallel)
library(doParallel)
test <-read.csv("pml-testing.csv")
train <- read.csv("pml-training.csv")
dim(train)
dim(test)
head(train)
tail(train)
head(test)
tail(test)
set.seed(123)
train <- train[,union(grep("^accel_", colnames(train)),grep("classe",colnames(train)) )]
test <- test[,union(grep("^accel_", colnames(test)),grep("classe",colnames(test)) )]
names(train)
train
head(train)
confusionMatrix(prediction1,s_test$classe)
clear
```r
library(ggplot2)
library(caret)
library(cluster)
library(survival)
library(randomForest)
library(Hmisc)
library(foreach)
library(iterators)
library(parallel)
library(doParallel)
```
library(ggplot2)
library(caret)
library(cluster)
library(survival)
library(randomForest)
library(Hmisc)
library(foreach)
library(iterators)
library(parallel)
library(doParallel)
test <-read.csv("pml-testing.csv")
train <- read.csv("pml-training.csv")
dim(train); dim(test)
set.seed(123)
train <- train[,union(grep("^accel_", colnames(train)),grep("classe",colnames(train)) )]
test <- test[,union(grep("^accel_", colnames(test)),grep("classe",colnames(test)) )]
names(train)
splitted <- createDataPartition(y = train$classe, p = 0.7, list = FALSE)
s_training <- train[splitted, ]
s_test <-  train[-splitted, ]
```r
s_training$classe <-as.factor(s_training$classe)
s_training[, 1:6] <- sapply(s_training[, 1:6], as.numeric)
s_test$classe <-as.factor(s_test$classe)
s_test[, 1:6] <- sapply(s_test[, 1:6], as.numeric)
```
s_training$classe <-as.factor(s_training$classe)
s_training[, 1:6] <- sapply(s_training[, 1:6], as.numeric)
s_test$classe <-as.factor(s_test$classe)
s_test[, 1:6] <- sapply(s_test[, 1:6], as.numeric)
registerDoParallel()
randomForest1 <- foreach(ntree=rep(150, 6), .combine=combine, .packages='randomForest') %dopar% {
randomForest(x=s_training[,1:12], y=s_training$classe, ntree=ntree)}
install.packages('e1071', dependencies=TRUE)
prediction1 <- predict(randomForest1,s_test)
confusionMatrix(prediction1,s_test$classe)
install.packages("e1071", dependencies = TRUE)
source('~/.active-rstudio-document', echo=TRUE)
# Weight Lifting Training Exercises Dataset
# Executive Summary
We have a data set based on the exercise performed by 6 young health male participants (20-28 years old), with little weight lifting experience.Everyone perfomed 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).
I've chosen RandomForest method with parallel processing because the large running time of the training set.
Read more in http://groupware.les.inf.puc-rio.br/har#ixzz3BJDO0kCC
#Libraries and data set load
```r
library(ggplot2)
library(caret)
library(cluster)
library(survival)
library(randomForest)
library(Hmisc)
library(foreach)
library(iterators)
library(parallel)
library(doParallel)
```
#Data set load
```r
test <-read.csv("pml-testing.csv")
train <- read.csv("pml-training.csv")
```
# Explore, cleaning and select data
Analyzing the data set briefly using head(), tail(), I've realized that many columns has NA, so I decide to select the main acceleration columns which names include "accel_"
```r
dim(train); dim(test)
set.seed(123)
train <- train[,union(grep("^accel_", colnames(train)),grep("classe",colnames(train)) )]
test <- test[,union(grep("^accel_", colnames(test)),grep("classe",colnames(test)) )]
names(train)
```
#Split the training data set 70%-30%
I've splitted the training set into training 70%  and test 30%
```r
splitted <- createDataPartition(y = train$classe, p = 0.7, list = FALSE)
s_training <- train[splitted, ]
s_test <-  train[-splitted, ]
```
#Convert column types
I convert the column classe as a factor and the other columns as a numeric
```r
s_training$classe <-as.factor(s_training$classe)
s_training[, 1:6] <- sapply(s_training[, 1:6], as.numeric)
s_test$classe <-as.factor(s_test$classe)
s_test[, 1:6] <- sapply(s_test[, 1:6], as.numeric)
```
#Model definition
I build a model using random forest and parallel processing to speed it up
```r
registerDoParallel()
randomForest1 <- foreach(ntree=rep(150, 6), .combine=combine, .packages='randomForest') %dopar% {
randomForest(x=s_training[,1:12], y=s_training$classe, ntree=ntree)}
```
#Prediction and Errors
I create a ConfusionMatrix using predict() but before I have to istall packages('e1071')
install.packages('e1071', dependencies=TRUE)
```r
install.packages('e1071', dependencies=TRUE)
prediction1 <- predict(randomForest1,s_test)
confusionMatrix(prediction1,s_test$classe)
```
# Answers
To predict the answera I use predict()
```r
predict(randomForest1, test)
```
#Conclusion
I achieve an accuracy of 93,9% so theorically almost all of the answers should be correct.
I found an error in answer 3. In order to solve it I must include more features.
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
s_test
prediction1 <- predict(randomForest1,s_test)
randomForest1
randomForest1 <- foreach(ntree=rep(150, 6), .combine=combine, .packages='randomForest') %dopar% {
randomForest(x=s_training[,1:12], y=s_training$classe, ntree=ntree)}
registerDoParallel()
randomForest1 <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {randomForest(x=sample1[,1:12], y=sample1$classe, ntree=ntree)}
prediction1 <- predict(randomForest1,s_test)
confusionMatrix(prediction1,s_test$classe)
source('~/.active-rstudio-document', echo=TRUE)
install.packages("e1071", dependencies = TRUE)
library(ggplot2)
library(caret)
library(cluster)
library(survival)
library(randomForest)
library(Hmisc)
library(foreach)
library(iterators)
library(parallel)
library(doParallel)
install.packages('e1071', dependencies=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
require(knitr)
install.packages("knitr")
knit2html('Project Machine Learning.Rmd')
library(datasets)
source('~/.active-rstudio-document', echo=TRUE)
